{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahekkothari/SLM_Summary/blob/main/SLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Goal: To create a SLM script that summarizes long texts that are written."
      ],
      "metadata": {
        "id": "rrF0tKSDT5rq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.probability import FreqDist\n",
        "from heapq import nlargest\n",
        "\n",
        "def summarize_text(text, num_sentences=3):\n",
        "    sentences = sent_tokenize(text) #will tokenize given text into sentences\n",
        "    words = word_tokenize(text.lower()) # will tokenize the text into words\n",
        "\n",
        "\n",
        "    stop_words = set(stopwords.words(\"english\")) # Remove stopwords aka commonly used words\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    word_freq = FreqDist(words) # finds the frequency of each word in the given text\n",
        "    sentence_scores = {}   # words recieve a score based on word frequency\n",
        "\n",
        "    for sentence in sentences:\n",
        "        for word in word_tokenize(sentence.lower()):\n",
        "            if word in word_freq:\n",
        "                if len(sentence.split(' ')) < 20: #Max amount of words\n",
        "                    if sentence not in sentence_scores:\n",
        "                        sentence_scores[sentence] = word_freq[word]\n",
        "                    else:\n",
        "                        sentence_scores[sentence] += word_freq[word]\n",
        "\n",
        "    # Select the top sentences with the highest scores\n",
        "    summarized_sentences = nlargest(num_sentences, sentence_scores, key=sentence_scores.get)\n",
        "\n",
        "    # Join the selected sentences to create the summary\n",
        "    summary = ' '.join(summarized_sentences)\n",
        "\n",
        "    return summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xBmwB3OGehNV",
        "outputId": "1a53b151-3ed4-48a7-d724-861b8d08c535"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "text = \"\"\"\n",
        "    We can view LLMs and SLMs as two ends of a spectrum with overlap in between. Overall SLMs distinguish themselves from LLMs in one or more of the following ways. SLMs are more fine-tuned because vendors or companies train them on detailed, domain-specific data, for example to assist complex data engineering tasks. They enrich user prompts, for example by injecting domain-specific data into a userâ€™s question to make the response more accurate. Data pipeline vendors are building SLMs with these capabilities now, often alongside LLMs, to help companies tackle specialized data engineering problems with better governance. This will help data teams boost productivity while reducing risks related to data quality, fairness, and explainability. We should get ready for a boom of small language models in data engineering and many other fields.\n",
        "- Kevin Petrie in Should AI Bots Build Your Data Pipelines? Part III: The Emergence of Small Language Models for Data Engineering June 21, 2023\n",
        "(Blog)\n",
        "\"\"\"\n",
        "summary = summarize_text(text)\n",
        "print(\"Summary:\")\n",
        "print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljOtrVYOqTG5",
        "outputId": "9e6c5038-d391-4e0e-c606-a80ff441ff4f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary:\n",
            "This will help data teams boost productivity while reducing risks related to data quality, fairness, and explainability. Part III: The Emergence of Small Language Models for Data Engineering June 21, 2023\n",
            "(Blog) We should get ready for a boom of small language models in data engineering and many other fields.\n"
          ]
        }
      ]
    }
  ]
}